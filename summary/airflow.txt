Summary

1. as we have realtime system, why batch system is needed
	to beat the CAP theorem
	http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html

2. Python globals() method return dict of current globals symbol table
	# Inject the properties into the __dict__ for this module. This will allow
	# airflow to pick up the DAGs.
	globals().update(dag_dict)

3. decorator in airflow 
	Function decorator that Looks for an argument named "default_args", and fills the unspecified arguments from it
    https://github.com/apache/incubator-airflow/blob/master/airflow/utils/decorators.py#L32-L88

4. use md5hash to distribute runtimes for jobs
    # value between 0 and 160 which is not intended to be perfectly uniform
    md5 = hashlib.md5()
    md5.update(str((cluster, db, table)).encode('utf8'))
	schedule_interval='{:d} */2 * * *'.format(int(md5.hexdigest(), 16) % 60)
	ref: https://en.wikipedia.org/wiki/Cron

5. airflow command
	ref: https://airflow.apache.org/cli.html

6. setup py3 env for airflow
	only use python 3.6 for venv3 with help from pyenv
	https://stackoverflow.com/a/32672975

7. try to delete apache_airflow.egg-info if exception throw 
	pkg_resources.ResolutionError: Script 'scripts/airflow' not found in metadata 
	at '/Users/hongyiw/dev/airflow/apache_airflow.egg-info'
	
8. error open airflow UI while swtich between PY2 & PY3
	solve by clear brower cookie, as PY2 and PY3 code store in cookie are different
	