Summary

1. How to run kcbq locally
	1) clone and fork kcbq repository then follow instruction below
		ref: https://github.com/wepay/kafka-connect-bigquery

	2) update standalone.properties
		bootstrap.servers=localhost:9092
		key.converter=io.confluent.connect.avro.AvroConverter
		key.converter.schema.registry.url=http://localhost:8081
		value.converter=io.confluent.connect.avro.AvroConverter
		value.converter.schema.registry.url=http://localhost:8081
		internal.key.converter=org.apache.kafka.connect.json.JsonConverter
		internal.value.converter=org.apache.kafka.connect.json.JsonConverter
		internal.key.converter.schemas.enable=false
		internal.value.converter.schemas.enable=false
		offset.storage.file.filename=/tmp/connect2.offsets
		rest.port = 8084

	3) update connector.properties
		name=bigquery-connector
		connector.class=com.wepay.kafka.connect.bigquery.BigQuerySinkConnector
		tasks.max=1

		topics=dbserver2.library.book
		sanitizeTopics=true

		autoCreateTables=true
		autoUpdateSchemas=true

		schemaRetriever=com.wepay.kafka.connect.bigquery.schemaregistry.schemaretriever.SchemaRegistrySchemaRetriever
		schemaRegistryLocation=http://localhost:8081

		bufferSize=100000
		maxWriteSize=10000
		tableWriteWait=1000

		########################################### Fill me in! ###########################################
		# The name of the BigQuery project to write to
		project=unique-acronym-194321
		# The name of the BigQuery dataset to write to (leave the '.*=' at the beginning, enter your
		# dataset after it)
		datasets=.*=library
		# The location of a BigQuery service account JSON key file
		keyfile=/Users/hongyiw/dev/airflow-dags/hongyi-airflow-demo-keyfile.json

	4) If you run multiple standalone instances on the same host, some settings must differ between each instance
		1) offset.storage.file.filename: storage for connector offsets, which are stored on the local filesystem in standalone mode
		
		2) rest.port: the port the REST interface listens on for HTTP requests

	5) Installing Connector Plugins by adding .jar to CLASSPATH
		ref: https://docs.confluent.io/3.1.1/connect/userguide.html#installing-connector-plugins

	6) ./connector.sh should load data to bigquery


2. how to run integration test for KCBQ
	follow the instructions: https://github.com/wepay/kafka-connect-bigquery#integration-testing-the-connector
	pay attention to:
		1) have docker running 
		2) put file 'test.conf' by 'integrationtest.sh' side  
		3) put file 'test.properties' by 'BigQueryConnectorIntegrationTest.java' side
		4) 'test.conf' is the same as 'test.properties' as follow
			project=unique-acronym-194321
			dataset=library
			keyfile=/Users/hongyiw/dev/airflow-dags/hongyi-airflow-demo-keyfile.json


3. how to release with Sonatype
	brew switch gnupg 1.4.22
	ref: https://github.com/wepay/kafka-connect-bigquery/wiki/Publishing-with-Sonatype
	ref: http://central.sonatype.org/pages/ossrh-guide.html
	ref: http://central.sonatype.org/pages/requirements.html
	ref: http://central.sonatype.org/pages/gradle.html
	ref: http://central.sonatype.org/pages/working-with-pgp-signatures.html
	ref: https://medium.com/@xiaoxiaohou/gpg-keyserver-receive-failed-no-route-to-host-9e47a0dd83f8
	ref: https://oss.sonatype.org/#nexus-search;quick~kcbq
	ref: https://mvnrepository.com/artifact/com.wepay.kcbq


4.  how to deal with compatible schema change
	1) delete existing table and view (be careful)
	2) devops turn off schema change limit 
	3) producer send a msg with new schema
	4) re-create views


5. how to deal with incompatible schema change
    1) delete existing table and view (be careful)
	2) truncate Kafka message for the topic
	3) push new schema for that topic
	4) re-bootstrap KCBQ
	5) re-create views


